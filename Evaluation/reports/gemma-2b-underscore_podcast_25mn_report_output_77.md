# Compte-rendu réunion 06/04/2024

## Table des matières

1. [Transcription de la réunion](#Transcription-de-la-réunion)
2. [Résumé de la réunion](#Résumé-de-la-réunion)
3. [Conclusion](#conclusion)

## Transcription de la réunion

<details>
<summary>View Full Transcription</summary>

Timestamp : [0.0, 442.88] / ['SPEAKER_03', 'SPEAKER_02', 'SPEAKER_01', 'SPEAKER_03', 'SPEAKER_02', 'SPEAKER_03', 'SPEAKER_01', 'SPEAKER_03', 'SPEAKER_01', 'SPEAKER_03']:<br> <br>**SPEAKER_03:** Vous le savez, ici on aime bien sélectionner des projets, des applis que les gens ne connaissent pas trop. Parce que depuis une année, on commence à avoir une maturation, une maturité de toutes ces nouvelles technologies d'intelligence artificielle, suffisamment pour que ça commence à devenir utilisable et intéressant pour le commun des mortels. Dans les applis que je vous ai sélectionnées, il y a à la fois des services, des applis que vous pouvez installer dès maintenant sur votre Mac pour gagner du temps, ainsi que des outils un peu plus avancés si vous voulez pousser le curseur. Si vous êtes dans la team des gens qui veulent, par exemple, commencer à améliorer des modèles vous-même, à faire des fine tuning, des choses comme ça. Je ne rentre pas trop dans le détail, mais je vais vous présenter quatre projets différents qui, je pense, vont vous faire halluciner. Le premier, il s'appelle Clap. C'est un service web à qui vous donnez une chaîne YouTube et à partir de ça, il vous crée des formats verticaux tout seul de A à Z. C'est ça que je trouve intéressant, c'est que j'avais déjà vu des démos avec des petites bribes, le fait de découper une vidéo en petits extraits ou le fait de rajouter automatiquement des sous-titres. J'avais vu toute cette petite brique, mais le côté vraiment de A à Z est assez impressionnant. Et comme vous voyez, on peut voir dans les démos, on peut voir du Hugo Décrypte. Je sais pas si c'est parce qu'il est vraiment client ou si c'est juste leur démo. Je pense que c'est leur démo, mais en vrai... C'est possible. C'est des Français qui font ça ? <br>**SPEAKER_02:** Ils sont hyper cool, c'est deux Frenchies qui font ce truc-là. Depuis que je me suis on est en contact tout le temps, on s'envoie des messages tous les jours et je suis un power user de leurs trucs de clap en fait parce que le podcast qu'on a avec William à l'ascenseur, après moi chaque semaine je l'envoie dans la moulinette de clap comme ça et il m'isole 10 potentiels sujets classés par ordre de viralité en fait parce qu'il analyse tout le transcript, il chope des passages viraux en gros, il me les sort comme ça, après Après, tu as un éditeur si tu veux rajouter un peu d'avant, un peu d'après, enlever quelques mots, etc. Tu peux tout faire et c'est trop bien foutu. Je leur ai suggéré des tonnes de features en tant que power user. Je leur disais, est-ce que vous pouvez rajouter ci, ça et ça ? Et ça, c'est trop bien quand c'est des gens accessibles comme ça. Les features que tu demandes, elles sont incluses dans le site. Ell qu'ils ont ajouté dernièrement qui est vraiment, je trouve, la clé de leur truc. Générer un clip par rapport à ce que vous vous souvenez que vous avez dit. Donc, tu lui dis j'ai parlé de ça vite fait dans le live, j'aimerais bien avoir ce passage. Et lui, en fait, il va le retrouver. Il va te faire le truc. Parce que parfois, tu te souviens que tu as parlé c'est très chiant de faire ça. Même les monteurs et tout, ils ont… enfin, c'est chiant. Ça fait <br>**SPEAKER_01:** plaisir à personne de monter des clips à partir d'un truc très grand, donc c'est cool. Et c'est d'ailleurs pour ça que nous, underscore, nous ne bien. On est quand même disponible, mais c'est pas nous. <br>**SPEAKER_03:** Eh ben, moi, je vous propose quelque chose. C'est qu'on fasse un test. Un test en direct. <br>**SPEAKER_02:** Oh là là. On n'a pas fait ça depuis 2018. Un test en direct, mais je vous dis, leur tool marche trop bien. On va pouvoir vérifier. Je vous propose Donc là, on colle le lien de la vidéo et hop, il trouve instantanément la bonne vignette, <br>**SPEAKER_03:** etc. Et nous, on va tout cocher. On lui dit que c'est du français, max une minute de durée et c'est parti. Et là, nous, pendant ce temps, on prend un petit cocktail, on attend quoi. Pendant que c'est en train de faire la génération, je vous propose de passer à l'appli suivante qui pour le coup est installable en local sur votre machine. Elle s'appelle Better Dictation, meilleure transcription. Ça va vous permettre de gagner énormément de temps sur l'envoi de vos messages, notamment de vos messages privés. Si vous êtes sur votre Mac, vous avez le choix entre taper au clavier vos messages ou alors utiliser la dictée vocale, mais personne ne fait ça parce que ça marche très mal. Voilà, tout ce qui est mots un petit peu complexes, langage, métier, globalement, ce n'est pas ouf et donc on a pris l'habitude de ne pas utiliser ça. Sauf qu'un truc que moi, j'ai réalisé, c'est que sur des longs messages, genre tu prends à partir de quatre, cinq lignes, à l'oral, tu peux mettre jusqu'à 5 fois moins de temps à composer ton message qu'à l'écrit. Il faut vraiment se rendre compte qu'il y a une différence énorme entre ton temps de dictée et ton temps d'écriture. Même si tu es le roi du clavier, tu vois. <br>**SPEAKER_01:** Or, on sait que depuis, on a des modèles de transcription type Whisper V3 <br>**SPEAKER_03:** qui sont hyper qualis. Mais jusqu'à présent, pour les installer, pour avoir un workflow, les installer pour avoir un workflow, une intégration agréable dans tes messageries, dans tes DM Twitter, dans ton Discord, ton Slack, tout ça. C'est un peu de galère. Jusqu'à Better Transcription. Le principe est hyper simple. Vous allez avoir un raccourci clavier que vous pouvez maintenir en même temps que vous parlez pour faire une transcription. Ce qui est cool, c'est que c'est un push to talk. C'est-à-dire que tu es en train de répondre à un DM sur n'importe quelle appli, tu maintiens ton raccourci clavier et tu te mets à parler. Et à partir du moment où tu as retiré ton doigt, il commence à transcrire ton message. Ce qu'il fait, c'est qu'il va mettre un petit listening avec trois petits points, peu importe là où est ton curseur, c'est ça qui est cool, c vous montrer comment c'est. Ceci est un test. Je suis actuellement en train de rédiger un très long message qui m'aurait pris vraiment très longtemps à écrire. Cette émission vient de commencer et je vois qu'ils sont en train de meubler pendant que moi, je prépare mes illustrations. Ce message est bien long. Regardez la qualité. Je crois qu'il n'y a pas une faute. Je ne vois pas une faute. Il là, vous avez vu, il y a eu un petit temps de latence. Ça vient du fait que j'utilise le plus gros des modèles. Si vraiment vous êtes quelqu'un de pressé, vous pouvez utiliser un modèle plus petit, Whisper Medium par exemple ou des choses comme ça. Là, moi, je veux que mes messages soient pixels perfects ou mes mails parce que c'est trop bien le problème mail. Ah oui, donc plus ça prend du temps, plus n'y a pas besoin d'internet. Du coup, le Whisper est téléchargé. Il n'y a pas besoin d'internet. <br>**SPEAKER_01:** Sur n'importe quelle appli. Combien de gigas le modèle ? <br>**SPEAKER_03:** 4 gigas peut-être ? Je crois que la génération est bientôt finie. Mais je vous propose de faire juste mon application suivante. On ira voir ça juste après. La suivante, elle s'appelle Oli Olliamma. Peut-être que vous en avez entendu parler. C'est le modèle de Meta, un des premiers concurrents de ChatGPT qu'on avait couvert, qui avait fait beaucoup de bruit il y a quelques mois. Depuis, il y a eu un florilège de nouveaux modèles. C'est très intéressant. La plupart d'entre vous, je pense, sont perdus parce qu'il y a trop de choses, trop de nouveautés tout le temps et vous en avez marre d'avoir sans vous, dites-vous que Oliyama, c'est l'appli parfaite si vous avez un Mac pour gérer vos modèles justement, pour enlever toute la friction sur le fait d'installer un nouveau modèle et de l'utiliser. Pourquoi ? Ce qu'il faut bien comprendre, c'est que le génie d'Oliyama, c'est qu'ils ont inventé une nouvelle syntaxe pour décrire un modèle, une IA qui tourne en local. C'est un peu une fondation qui leur permet après d'avoir une gestion des modèles. Si vous n'êtes pas développeur, vous vous demandez bien à quoi ça peut servir. En une simple ligne de commande, on fait « oliamarun mistral » par exemple, dans son terminal. Et hop, on se retrouve à parler avec Mistral. C'est tout. Et c'est lui qui s'est chargé automatiquement d'aller télécharger le modèle dans sa dernière version, de l'installer en local et de garder un serveur toujours ouvert derrière. Vous pouvez avoir accès à une liste très longue de modèles qui sont mis à jour constamment. Et le plus intéressant, c'est que c'est un service qui tourne toujours en tâche de fond. Si vous avez testé quelques applications pour utiliser des chats GPT locaux, comme Studio LM, il y en a plusieurs comme ça qui sont très bien. <br> 

Timestamp : [435.52, 894.22] / ['SPEAKER_03', 'SPEAKER_01', 'SPEAKER_03', 'SPEAKER_01', 'SPEAKER_03', 'SPEAKER_01', 'SPEAKER_03', 'SPEAKER_02', 'SPEAKER_00', 'SPEAKER_02', 'SPEAKER_03', 'SPEAKER_02', 'SPEAKER_03']:<br> <br>**SPEAKER_03:** Si vous avez testé quelques applications pour utiliser des chats GPT locaux, comme Studio LM, il y en a plusieurs comme ça qui sont très bien. et d'avoir des copies de la petite brique qui fait les inférences. Et ça, c'est du gâchis, c'est hyper dommage. Et surtout, ce n'est pas du tout pérenne comme système. Au Olliamma, ils arrivent et disent « non mais on arrête tout ça maintenant, maintenant tout passe par nous ». Je ne sais pas si vous voyez la distinction, c'est plutôt que chacun ait sa propre rubrique, tu as un service qui tourne constamment, tu le vois dans ta petite barre de menu sur ton Mac, qui gère les mises à jour de modèle, qui gère cette maintenance-là et toutes les autres applis se mettent à discuter avec. Et c'est en train de parce que c'est une fondation super robuste. Ce n'est pas un truc Python un peu cracra, c'est fait en Go. Le truc est propre et c'est une bonne fondation pour tous les développeurs et les utilisateurs qui veulent utiliser de l'IA en local. Vous allez me dire, c'est cool ton truc, mais qu'est-ce qu'on peut faire maintenant ? Ce que je vous propose, c'est de un launcher, une alternative à la barre de recherche sur macOS. Encore une fois, vous allez penser que cette barre de recherche sur Mac marche très bien, mais il faut vraiment voir Raycast comme une tour de contrôle qui vous permet d'interagir avec toutes vos applications et de lier des raccourcis clavier, de développer des modules personnalisés sur votre Mac. En fait, c'est l'outil de productivité ultime des gens qui veulent être plus fast sur Mac tout simplement. Moi je l'utilise depuis très longtemps aussi <br>**SPEAKER_01:** c'est incroyable. Ne pas faire une erreur, c'est vouloir tout comprendre à Ecast au premier coup il faut se laisser surprendre, tu vas pouvoir potentiellement découvrir des fonctionnalités dans un an et c'est très bien. Parce que si tu cherches à tout comprendre au début tu vas être submergé. Par exemple un at at et ça me met la première adresse mail, ça me remplit automatiquement en fait ça peut te faire des raccourcis comme ça Pour ceux qui connaissent Alfred, c'est une alternative <br>**SPEAKER_03:** moderne qui résout pas mal de problèmes qu'il y avait et effectivement t'as des snippets comme ça où tu peux gagner du temps. Tes fenêtres tu peux les mettre <br>**SPEAKER_01:** où tu veux en un raccourci T'as un historique de ton presse-papier donc si t Tu as copié-collé un truc il y a trois jours. C'est incroyable. Hop, tu recherches dans ta liste et tu retrouves. Si vous êtes du genre à vous créer des petites automatisations perso, <br>**SPEAKER_03:** mais que vous avez envie d'une belle interface par-dessus, plutôt que de réinventer la roue, vous utilisez Recast et ça vous fait une interface parfaite sur toutes vos automatisations, vos scripts. Et potentiellement, si vous voulez automatiser des choses avec des modèles locaux, vous pouvez combiner Raycast et Olliamin. Et là, ça devient dingue. Je vous ai fait une petite vidéo de démonstration avec deux exemples vraiment hyper simples. Premier exemple, imaginez que vous avez envie de faire rédiger des mails sans avoir à vous emmerder. Par exemple, vous êtes en retard pour vos clients ou vous êtes en retard au travail, vous avez besoin d'écrire un mail formel, mais vous, vous avez un peu la flemme et vous savez que maintenant, ça se fait très bien. Quel est le moyen le plus rapide de faire ça avec son clavier ? Ce que vous créez, c'est un raccourci recast qui va appeler Olliamma sur un modèle local pour vous fournir une réponse et la mettre dans votre presse papier. Mon cerveau est trop petit pour imaginer tout ce qui est possible de faire avec ça. Et si vous, vous avez des workflows, des idées, justement, donnez-les-nous parce que c'est vraiment un bac à sable de fou. La première démo que je vous montre, donc, c'est le process. Donc ça, c'est la fenêtre de Recast dans laquelle on peut créer une commande custom. C'est la première fonction. Et donc, c'est là 4 secondes à créer, ce qui est quand même un argument important. Et je lui demande juste de signer mon mail avec une petite signature des familles. Quel CEO microp ! Une fois que c'est fait, on crée l'extension. Donc là, je viens de faire un commande de rentrée et donc ça crée le raccourci que j'appelle « rédige un mail, please ». <br>**SPEAKER_01:** Olivier Roland Donc là, en fait, tu paramètres ta commande pour pouvoir l'utiliser plus tard. Thomas Lecce Exactement. Olivier Roland Parce que je n'ai pas compris. Thomas Lecce Ça, c'est le paramétrage de la commande. Une fois que c'est fait, je peux texte, j'invoque « rédige-moi un mail » et pam, il se met à me rédiger un long mail formel pour expliquer à mon entreprise pourquoi je suis en <br>**SPEAKER_03:** retard. Là, il y va ! Il faut dire qu'il est quand même bien verbeux. « Nous travaillons actuellement avec toutes nos ressources pour résoudre ce problème et résoudre le temps de retard ». Le temps que ça m'aurait pris de rédiger cette excuse. Là, tu te fais virer si tu envoies un mail aussi long à ton boss. Alors que tu es en retard, tu es foutu. Évidemment, ce template est perfectible. Vous avez vu, il ne fait qu'une phrase. Il suffirait de l'améliorer un petit peu. Ce qui est cool, c'est qu'une fois que c'est fait, c'est un système pour corriger vos fautes d'orthographe automatiquement. Par exemple, vous rédigez une phrase pleine de fautes d'orthographe. Pareil, juste vous la sélectionnez, raccourci, récaste, corrige-moi mes fautes d'orthographe. Voilà, la phrase était juste plein de fautes et il me l'a changé en plein de défauts. Il se trouve que mon prompt n'est Même chose en 4 secondes, vous pouvez même le binder sur un raccourci clavier. Vous êtes en train de rédiger un mail, vous faites sélection, raccourci clavier, toutes vos fautes sont corrigées avec une vraie intelligence, ce n'est pas un autocorrecteur de merde. Tout ça sans aucune friction, en local, sans internet, c'est quand même de la magie noire. Oui, de ouf. Je vous propose de maintenant les résultats. Smartphone DIY à l'aide de composants open source. Ok, donc il a bien compris le sujet. Et là, il nous met un score de viralité de 80 sur 100. Est-ce que c'est bien ou pas, Benjamin ? <br>**SPEAKER_02:** Oui, 80, c'est pas mal, surtout qu'en général, il dit « ouais. C'est logique. Par contre, à l'heure des charges, vous avez un cadrage un peu vraiment spécial pour vos trucs. Donc là, son cadrage, il l'a fait parce qu'ils ont des cadrages automatiques qui marchent bien la majeure partie du temps. Mais vous, avec votre layout, il y a moyen. Voyons voir ce que ça donne. <br>**SPEAKER_00:** Ça, c'est un truc qui est faitrimante 3d et que j'achète les bons composants je peux refaire le téléphone exactement et normalement chargé on va voir si jamais il va démarrer C'est juste très zoomé interface est très cool voilà il ya franchement c'est joli il faut alors il faut taper avec des ongles parce que c'est un écran ongle parce que c'est un écran résistif et Et oui, malheureusement, c'est vraiment beaucoup moins cher. Il y a rien eu. C'est trop fort. Bon là, je vais peut-être couper un peu avant la fin. <br>**SPEAKER_02:** Regarde, ben justement. Attends, c'était pas encore fini. Non, mais clique sur Edit Export maintenant. Là, tu vas dans Vidéo, en fait. Le tag Vidéo, là en haut à gauche, voilà, ici. Et là, du coup, Mike, tu peux rajouter du texte au-dessus ou tu peux en enlever en dessous en fait. Ah ouais, comme des scripts ! Tu viens retravailler un petit peu ton script. Mais sur le texte directement ? Ouais, là sur le script, clique sur le texte que tu as sélectionné. Il va te proposer de faire « Add » par exemple, tu vois. En bas, tu as une timeline, tu peux agrand'onglet crop là-haut. Pour chaque tronçon, t'as tes crop et tu peux choisir. Et ça, c'est ton cadrage en fait. Voilà. Donc tu peux retravailler chaque plan. <br>**SPEAKER_03:** Et surtout pour préciser, le choix des phrases de la première phrase du TikTok, c'est le moment le plus important. C'est ça qui fait tout dans cet algorithme fou. C'est super bien pensé, je trouve, comme début d'histoire, en sachant que nos vidéos ne s'y prêtent pas forcément, parce que souvent, on fait des longs tunnels, des discussions qui évoquent un même sujet pendant très longtemps. Et j'avoue, je suis assez épaté du résultat. C'est bluffant. On va le publier sur TikTok. <br>**SPEAKER_02:** Moi, ce que je dis de mon expérience perso du truc, malgré tout simplifie déjà tellement la tâche de base. Sans plus attendre, je vous propose de passer à l'outil suivant. <br>**SPEAKER_03:** C'est sûr que vous avez déjà eu besoin d'un truc de ce genre par le passé et vous <br> 

Timestamp : [893.94, 1334.5] / ['SPEAKER_03', 'SPEAKER_01']:<br> <br>**SPEAKER_03:** On va le publier sur TikTok. Moi, ce que je dis de mon expérience perso du truc, malgré tout simplifie déjà tellement la tâche de base. Sans plus attendre, je vous propose de passer à l'outil suivant. C'est sûr que vous avez déjà eu besoin d'un truc de ce genre par le passé et vous pour faire ça alors que ça ne paraît pas si compliqué. Vous connaissez tous Remove BG par exemple ou des trucs comme ça. Ce n'est que des services qui vous proposent d'enlever le fond d'une image, de faire un détourage de qualité. Avant, c'était horrible. Maintenant, avec l'IA, c'est moins horrible. Mais je n'ai pas compris pourquoi pendant encore des années, il y avait beaucoup de services, mais ils étaient tous un peu complexes et ils te faisaient souvent très payer si tu voulais avoir de la bonne résolution. Eh bien, j'ai une annonce à vous faire si vous êtes concerné. Un modèle ouvert et gratuit et sorti qui permet de faire du détourage super quali. Et tout le monde peut l'utiliser là de chez vous, directement dans votre navigateur avec cette petite démonstration de Hugging Face. Ce n'est pas le plus impressionnant dans le sens où c'est de l'édition d'image, ça fait longtemps qu'on sait le faire. C'est quand même bluffant par la qualité du résultat. Donc là, vous pouvez voir que les petites ombres, les petits cheveux, tout ça est super bien détouré. J'étais heureux, j'ai vu ça, je me suis dit génial, encore une nouvelle brique dans l'arsenal open source qu'on a tous à notre dispo maintenant. Mon projet suivant pour le coup est bien plus complexe. Là, vous allez voir, on commence à rentrer dans des outils qui vont seulement intéresser les plus motivés d'entre vous et ceux qui ont des grosses envies d'automatisation. Peut-être que vous avez déjà vu des interfaces nodales qui permettent de créer des sortes de graphes logiques entre différents nœuds. C'est la fameuse programmation visuelle. Je vous avais parlé de N8N à une certaine époque, mais c'est tous ces systèmes qui permettent à n'importe qui de programmer avec une interface graphique. C'est plus joli. Moi, je comprends, ça permet d'aller un peu plus vite si on ne s'est pas développé, etc., même si on sait d'ailleurs. Et depuis un certain temps, des interfaces de ce type-là existent pour l'IA. En gros, la promesse, c'est de vous dire qu'on tu fais ce que tu veux, tu peux connecter tous ces nœuds ensemble pour automatiser ton travail et partir au Bahamas. Le problème, spoiler, la plupart de ces outils sont nuls. En tout cas moi je ne les aime pas. Non mais je vais m'expliquer, je vais défendre ma tech. Mais typiquement des lang flow ou des choses comme ça, on peut vous montrer à quoi ça ressemble. Ils ont toujours des pages, des landing pages super sexy ces gens. Ils ont toujours des interfaces qui sont très léchées et pourtant, moi je défends que ça ne me satisfait pas. Pourquoi ? Parce qu'en général, on va devoir rentrer un petit peu dans la technique, mais vous allez voir, c'est intéressant. Ils sont basés sur des gros frameworks comme on appelle. C'est des librairies pour développeurs qui sont en Python et qui sont un peu tentaculaires, qui essayent de tout faire en même temps, comme longchain par exemple. C'est très bien, c'est un bon bac à sable, mais le gros problème de créer des systèmes comme ça, c'est là, vous allez passer du temps, vous allez travailler, vous allez créer des automatisations qui ont potentiellement beaucoup de valeur pour vous, pour votre travail, pour votre entreprise ou juste pour vous, pour gérer votre vie. Donc, vous avez envie de construire sur du rock, pas sur un truc un peu pété qui risque d'exploser à la prochaine mise à jour ou qui marche une fois sur trois. Le problème de la plupart de ces outils, c'est que du coup, ils sont basés sur Python. Pourquoi c'est un problème ? Parce que Python a été utilisé dans l'IA et est devenu le langage par défaut de l'IA parce qu'il est extrêmement compréhensible et facile à prendre en main. Rien ne ressemble plus à de l'anglais que Python finalement. Donc, pour une audience de chercheurs en intelligence artificielle qui ne sont pas des développeurs, c'est parfait pour qu'ils puissent manipuler les concepts qu'ils connaissent, mathématiques ou de recherche, sans avoir la lourdeur des langages plus bas niveau. Pour faire tourner des modèles d'IA sur des serveurs dans le cloud par exemple, ça ne pose pas vraiment de problème. On sait gérer cette complexité de déployer des modèles de Python, des choses comme ça. Mais pour l'ère d'aujourd'hui qui, je le répète, est l'IA locale, ce n'est pas adapté du tout. Python n'est pas du tout fait pour être déployé sur des appareils en local à grande échelle. Ça n'a pas vraiment été pensé pour ça. Il y a des stratégies qui permettent de… Là, typiquement, dans les applis qu'on a cités, ils auraient pu essayer de tordre le langage, on va dire, d'utiliser des technos spécifiques genre, je ne sais plus comment elle s'appelle, PINSTALL ou des choses comme ça, qui permettent de faire une sorte de gros sac où on met Python littéralement, le langage ent pas le serpent et votre code. En gros, c'est dégueulasse. C'est possible, mais c'est dégueulasse. Il ne faut pas faire ça. Et donc, on se retrouve dans une situation terrible qui doit vous concerner si vous êtes intéressé par l'IA et l'IA locale. Il y a plein de nouveaux modèles partout. Tout le monde a des idées d'applis, de choses à faire, mais on n'a pas du tout la base, l'architecture qui permet de proprement déployer des applis de ce genre à grande échelle sur des machines de façon optimisée. Il y a un vrai manque à ce niveau-là. Aujourd'hui, tu veux commencer en tant que développeur à créer une appli révolutionnaire pour Mac ou pas, ou sur Windows, pour révolutionner je ne sais pas quoi avec de l'IA locale, tu es un peu niqué et tu ne peux pas utiliser des applications comme ça, des systèmes de graphes qui vont te faciliter la vie. Cette longue introduction était importante pour vous présenter le projet du jour, Floneum, ça s'appelle. Alors Floneum, vous allez voir, attention, je vous préviens, ça ne paye pas de mine. Donc l'interface est immonde. Attendez à dire. Tu l'as tellement. Non, mais objectivement, l'interface est assez infâme à utiliser. Comme vous pouvez le voir, on garde ce principe de graphe à gauche sur lesquels on peut éditer des entrées, des sorties, des choses comme ça. Par exemple, tu peux avoir demandé à un mistral de générer du texte en local. Après, tu peux demander de faire une recherche sur Google, toujours en local. C'est-à-dire que c'est vraiment le logiciel qui fait la recherche en arrière-plan. Tu peux demander d'ouvrir Chrome, par exemple, et de faire de la transcription, tu peux demander d'analyser une image. Bref, tous les fondamentaux de l'IA moderne. Pourquoi ça, c'est différent de tout le reste que je vous ai montré ? Et pourquoi, en réalité, ce n'est pas cette interface qui est le plus important, mais c'est ce qu'il y a derrière qui, je pense, pourrait intéresser énormément de développeurs par la suite. C'est que tout ça est développé en Rust. Ah, la passion du Rust qui revient. Tout ça se passe de manière 100% locale avec, je vous l'ai dit, des technologies faites pour être utilisées sur des appareils de tout à chacun de façon optimisée. Demain, toi, tu peux peut-être inventer un nouveau module qui permet, avec une IA, de détecter s'il y a un sponsor dans une vidéo, par exemple. Tu pourrais publier un module que tous les utilisateurs de Flonium pourraient intégrer dans leur graphe. Je vous passe les détails, mais si vous êtes un développeur sur ce genre de techno, il y a tout ce qu'il vous faut, des systèmes de base de données qui vous permettent d'ingérer des articles, des PDF, des choses comme ça. Bref, c'est un bac à sable assez complet. Je ne vais pas vous mentir, c'est une alpha. C'est vraiment une alpha de alpha, ne pensez pas pouvoir l'utiliser là demain. C'est un truc à surveiller, à se mettre dans les favoris et à voir dans six mois à quoi ça va ressembler. Parce que ce qui est le plus intéressant dans le projet, ce n'est pas cette interface par-dessus, c'est tout ce qu'il y a derrière en gros. C'est toutes les briques qui sont utilisables par tous les développeurs et qui m d'ailleurs, vous voulez intégrer des fonctionnalités locales d'IA, utilisez sa librairie à lui et vous allez gagner un temps fou. Très très chouette. <br>**SPEAKER_01:** Cool hein ? Ça m'a fait beaucoup rire de… à un moment j'ai regardé le chat et j'ai fait « Rust, incroyable ! » et tout. <br> 

Timestamp : [1329.68, 1537.14] / ['SPEAKER_03', 'SPEAKER_01', 'SPEAKER_03', 'SPEAKER_01', 'SPEAKER_03']:<br> <br>**SPEAKER_03:** Très très chouette. Cool hein ? Ça m'a fait beaucoup rire de… à un moment j'ai regardé le chat et j'ai fait « Rust, incroyable ! » et tout. et que vous trouvez que les LLM ne sont pas au niveau pour telle ou telle tâche que vous voudriez automatiser. Quand vous arrivez à ce moment-là, c'est qu'il faut commencer à faire des fine-tunes. Mais peut-être que vous vous pour qu'ils deviennent encore plus forts. Il n'y a pas nécessairement besoin de GPU immenses, notamment grâce à une technique que les gens qui font du stable diffusion ou des choses comme ça connaissent très bien. Ça s'appelle le Qlora. Ça permet sur un GPU de petite taille de faire une amélioration, un fine tuning de n'importe quel modèle avec ses propres données. Le projet dont je vais vous parler là maintenant, il vous prend vraiment par la main dès le début. C'est-à-dire que là, vous maintenant, vous n'avez jamais fait ça de votre vie, vous n'êtes pas des ingénieurs en intelligence artificielle, et pourtant, grâce à ça, grâce à Unsloth, vous allez pouvoir partir de zéro et créer votre propre modèle fine-tuné sur vos données. Beaucoup d'entreprises constatent que parfois, finetuner un petit modèle sur des tâches précises leur donne les mêmes performances que GPT-4. Mais il y a une barrière à l'entrée, il y a une complexité qui, avec ce projet, est complètement enlevée. Ils vous prennent vraiment par la main, même si vous n'y connaissez rien, et vont vous expliquer comment générer des données qui ont le bon format pour entraîner votre propre IA. IA. Ils vous ont préparé des Google Collabs. C'est un système de Google qui permet de faire tourner du code sur des GPU que vous pouvez louer facilement sans même avoir d'ordi très puissant. Tout ça est préparé pour vous baliser. Il y a tous les tutos, tout est expliqué. Et surtout, ils ont optimisé à mort le processus de fine tuning d'apprentissage. Il y a eu pas mal de benchmark qui ont été faits entre le process lambda habituel, les librairies standards pour faire du fine tuning, et Ensloth. Et ils arrivent à faire des x1,5 ou x3 sur les vitesses d'apprentissage, combinées à des besoins en VRAM qui sont beaucoup plus petits. Ce qui vous limite le plus rapidement quand vous faites de l'IA, c'est la quantité de mémoire de vos GPU, pas nécessairement leur puissance. Parce que à la limite si tu as un GPU qui est un peu moins puissant, tu peux juste attendre plus longtemps finalement. La mémoire, c'est le truc qui limite tout le monde et qui fait qu'on est tous là à ne pas pouvoir faire tourner les meilleurs modèles parce qu'on n'a pas des cartes graphiques avec 40 gigas. La solution pour ça, c'est d'optimiser en utilisant Unsloth qui vous demand de mémoire pour faire un même entraînement. Le chat posait une question importante, à savoir le prix. Tout ça est parfaitement open source. Vous pouvez le faire tourner sur vos propres machines ou alors sur un Google Collab. Ils ont une version pro, je crois, où ils poussent encore plus loin certaines optimisations, etc. Olivier Roland Il peut le faire tourner pour nous, non ? Est-ce qu'ils ont des cartes <br>**SPEAKER_01:** graphiques en location ? Olivier Roland C'est le Hunchloss Pro, justement ils vont pousser encore les optis et ils vont vous accompagner encore plus loin avec le repo GitHub tout simplement <br>**SPEAKER_03:** ça c'est incroyable, c'est à dire qu'ils peuvent tester avant de <br>**SPEAKER_01:** faire de la merde avec des petits documents comme ça vous n'avez plus d'excuses <br>**SPEAKER_03:** je veux voir des modèles créés par la communauté je veux voir des culorats spécialisés à faire des choses variées faites des trucs par exemple en français je trouve qu'il manque pas mal de modèles français. Cette barrière à l'entrée a trop longtemps limité le potentiel des modèles fine-tunés. Elle a été levée. Il ne tient qu'à vous maintenant de faire des modèles entraînés sur des tâches précises qui explosent OpenAI. C'est parti, à vous de jouer. <br> 

</details>

## Résumé de la réunion

### Noms des participants

-SPEAKER_02: ...
-SPEAKER_00: ...
-SPEAKER_01: ...
-SPEAKER_03: ...
### Sections

# Rapport sur les résumés

**Introduction**

* Introduction des intervenants et des objectifs du rapport.
* Description des sujets principaux :
    * Clap
    * Whisper
    * Oliyama
    * ChatGPT

**Chapitre 1 : Introduction à la technologie de IA**

* Définition de l'IA et son importance dans le domaine numérique.
* Introduction aux technologies de IA et leurs applications.
* Description de la transcription audio et son rôle dans le rapport.

**Chapitre 2 : Clap**

* Description de la plateforme et son fonctionnement.
* Méthode de génération de formats verticaux.
* Cas d'utilisation du Clap.

**Chapitre 3 : Whisper**

* Introduction de Whisper et son modèle.
* Syntaxe de Whisper et nouvelles fonctionnalités.
* Cas d'utilisation du Whisper.

**Chapitre 4 : Oliyama**

* Introduction de Oliyama et son modèle.
* Syntaxe d'Oliyama et nouvelles fonctionnalités.
* Cas d'utilisation de l'Oliyama.

**Chapitre 5 : ChatGPT**

* Introduction de ChatGPT et son modèle.
* Syntaxe de ChatGPT et nouvelles fonctionnalités.
* Cas d'utilisation de ChatGPT.

**Chapitre 6 : Python**

* Introduction de Python et son rôle dans l'IA.
* Description des outils et extensions Python pour l'automatisation des tâches.
* Cas d'utilisation de Python pour l'automatisation.

**Chapitre 7 : Introduction à Floneum**

* Définition de Floneum et son rôle dans le développement de l'IA locale.
* Description de l'interface de Floneum et ses fonctionnalités.
* Cas d'utilisation de Floneum.

**Chapitre 8 : Applications de Floneum**

* Applications mobiles
* Applications web
* Applications graphiques
* Applications de machine learning

